{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb3e966",
   "metadata": {},
   "source": [
    "# 1-create-clean-demographics-noteook - Plan\n",
    "This notebook is for processing the demographic dataset into a usable format. This is the first step in our process because when we come to cleaning and processing the other data that we have (notebook 2 to 5), we want to remove unrealistic data for example, events that happen before 1910 or in the future. \n",
    "\n",
    "In this first notebook, we take in 2 different reference datasets that allow us to map demographic information (age and sex) to nhs_numbers. \n",
    "\n",
    "1) Read the two datasets into a DemographicDataset \n",
    "2) Process this - meaning clean and make it uniform and then save in an arrow format. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6712ebb0",
   "metadata": {},
   "source": [
    "# Locations/paths naming convention\n",
    "\n",
    "1. We do not use relative paths.\n",
    "2. We do not explicitly use the word FOLDER in the naming, so `MEGADATA_LOCATION`, not `MEGADATA_FOLDER_LOCATION`. \n",
    "1. Location and path are in `UPPER_CASE`.\n",
    "2. When referred to as `_LOCATION`, the variable contain a string with the path.\n",
    "3. When referred to as `_PATH`, the variable is an `AnyPath` path object.\n",
    "4. The folder order is \"what it is\" / \"Where it's from\" so, for example megadata/primary_care not primary_care/megadata; so `MEGADATA_PRIMARY_CARE_LOCATION` or `PROCESSED_DATASETS_PRIMARY_CARE_PATH`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76797a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = \"version010_2025_05_SR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab471ae",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7dc7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tretools.datasets.demographic_dataset import DemographicDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0ec694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tretools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732501ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pathlib\n",
    "from cloudpathlib import AnyPath\n",
    "from datetime import datetime\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef272f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# polars namespace additions\n",
    "\n",
    "# In subsequent version this code may be integrated with the establisted TRE Tools package\n",
    "\n",
    "@pl.api.register_lazyframe_namespace(\"TRE\")\n",
    "class TRETools:\n",
    "    def __init__(self, lzdf: pl.LazyFrame) -> None:\n",
    "        self._lzdf = lzdf\n",
    "        \n",
    "    def unique_with_logging(self, *args, label: str = \"Unique\", **kwargs) -> pl.LazyFrame:\n",
    "        before = self._lzdf.select(pl.first()).collect().height\n",
    "        filtered_lzdf = self._lzdf.unique(*args, **kwargs)\n",
    "        after = filtered_lzdf.select(pl.first()).collect().height\n",
    "        \n",
    "        if before > 0:\n",
    "            change = ((after - before) / before) * 100\n",
    "            change_str = f\" ({'+' if change > 0 else ''}{change:.1f}%)\"\n",
    "        \n",
    "        unchanged = \" (row count unchanged)\" if after == before else \"\"\n",
    "        \n",
    "        print(f\"[{label}: on {args}] Before unique: {before} rows, After unique: {after} rows{unchanged}{change_str}\")\n",
    "        return filtered_lzdf    \n",
    "    \n",
    "    def filter_with_logging(self, *args, label: str = \"Filter\", **kwargs) -> pl.LazyFrame:\n",
    "        before = self._lzdf.select(pl.first()).collect().height\n",
    "        filtered_lzdf = self._lzdf.filter(*args, **kwargs)\n",
    "        after = filtered_lzdf.select(pl.first()).collect().height\n",
    "        \n",
    "        if before > 0:\n",
    "            change = ((after - before) / before) * 100\n",
    "            change_str = f\" ({'+' if change > 0 else ''}{change:.1f}%)\"\n",
    "        \n",
    "        unchanged = \" (row count unchanged)\" if after == before else \"\"\n",
    "        print(f\"[{label}] Before filter: {before} rows, After filter: {after} rows{unchanged}{change_str}\")\n",
    "        return filtered_lzdf\n",
    "    \n",
    "    def join_with_logging(\n",
    "        self,\n",
    "        other: pl.LazyFrame,\n",
    "        *args,\n",
    "        how: str = \"inner\",\n",
    "        label: str = \"Join\",\n",
    "        **kwargs\n",
    "    ) -> pl.LazyFrame:\n",
    "        left_before = self._lzdf.select(pl.first()).collect().height\n",
    "        right_before = other.select(pl.first()).collect().height\n",
    "        joined_lzdf = self._lzdf.join(other, *args, how=how, **kwargs)\n",
    "        after = joined_lzdf.select(pl.first()).collect().height\n",
    "        \n",
    "        if left_before > 0:\n",
    "            change = ((after - left_before) / left_before) * 100\n",
    "            change_str = f\" ({'+' if change > 0 else ''}{change:.1f}%)\" if abs(change) > 1.00 else f\" ({after - left_before} rows {' removed' if change < 0 else ' added'})\"\n",
    "        \n",
    "        unchanged = \" (row count unchanged)\" if after == left_before else \"\"\n",
    "        print(f\"[{label}] Join type: {how.upper()}\")\n",
    "        print(f\"[{label}] Left: {left_before} rows, Right: {right_before} rows -> After: {after} rows{unchanged}{change_str}\")\n",
    "        return joined_lzdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d328737",
   "metadata": {},
   "source": [
    "### Scripting for automated next notebook initation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5a8595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Javascript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f748f7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def redirect_to_next_notebook_in_pipeline(other_notebook):\n",
    "    \n",
    "    js_code = f\"\"\"\n",
    "    if (typeof Jupyter !== 'undefined' && Jupyter.notebook && Jupyter.notebook.kernel) {{\n",
    "        // only runs when cell is executed, not from cached output\n",
    "        console.log(\"Redirecting to next notebook in pipeline...\");\n",
    "        Jupyter.notebook.save_checkpoint();\n",
    "        Jupyter.notebook.session.delete();\n",
    "        \n",
    "        setTimeout(function() {{\n",
    "            window.location.href = '{other_notebook}.ipynb';\n",
    "        }}, 1500)\n",
    "    }} else {{\n",
    "        console.log(\"Found cached output. Not an active notebook context. Skipping redirect.\")\n",
    "    }}\n",
    "    \"\"\"\n",
    "    display(Javascript(js_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2a6d30",
   "metadata": {},
   "source": [
    "### Make paths\n",
    "\n",
    "Make the folders for the results and a variable for each of the files with the path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e570dcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_LOCATION = \"/home/ivm/BI_PY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf54925",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PROCESSED_DATASETS_LOCATION =  f\"{ROOT_LOCATION}/{VERSION}/processed_datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a5aada",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMOGRAPHICS_LOCATION = f\"{PROCESSED_DATASETS_LOCATION}/demographics\"\n",
    "AnyPath(DEMOGRAPHICS_LOCATION).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff337e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPPING_FILE_LOCATION = (\n",
    "    \"/genesandhealth/library-red/genesandhealth/\"\n",
    "    \"2025_02_10__MegaLinkage_forTRE.csv\" # actually tab-delimited\n",
    ")\n",
    "\n",
    "DEMOGRAPHICS_FILE_LOCATION = (\n",
    "    \"/genesandhealth/library-red/genesandhealth/phenotypes_rawdata/\"\n",
    "    \"QMUL__Stage1Questionnaire/2025_04_25__S1QSTredacted.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dff9f1",
   "metadata": {},
   "source": [
    "### Load the data \n",
    "\n",
    "Here we are loading the data into the DemographicDataset object. We then define a config. This config is a simple way of pointing out the key columns in each of the mapping files so that we can process the data in a clean way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03856de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics = DemographicDataset(\n",
    "    path_to_mapping_file=MAPPING_FILE_LOCATION,\n",
    "    path_to_demographic_file=DEMOGRAPHICS_FILE_LOCATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ded112",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This step creates DOBs for all people who have a questionnaire (with an Oragene_ID)\n",
    "demographics.demographics = (\n",
    "    demographics.demographics\n",
    "    .lazy()\n",
    "    .select(\n",
    "        pl.col(\"S1QST_Oragene_ID\"),\n",
    "        pl.col(\"S1QST_Gender\"),\n",
    "        pl.col(\"S1QST_MM-YYYY_ofBirth\"),\n",
    "    )\n",
    "    .TRE\n",
    "    .filter_with_logging(\n",
    "        pl.col(\"S1QST_MM-YYYY_ofBirth\").ne(\"NA\"),\n",
    "        label=\"EXCLUDING `NA` DATE\"\n",
    "    )\n",
    "    .TRE\n",
    "    .unique_with_logging(\n",
    "        subset=[\"S1QST_Oragene_ID\"],\n",
    "        label=\"Check for repeated rows of matching `S1QST_Oragene_ID`\"\n",
    "    )    \n",
    "    .select(\n",
    "        pl.col(\"S1QST_Oragene_ID\"),\n",
    "        pl.col(\"S1QST_MM-YYYY_ofBirth\"),\n",
    "        pl.col(\"S1QST_Gender\"),\n",
    "    )\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f50a9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics.mapped_data = (\n",
    "    demographics.mapped_data\n",
    "    .lazy()\n",
    "    .TRE\n",
    "    .filter_with_logging(\n",
    "        pl.col(\"55273exomes_release_2024-OCT-08\").is_not_null(),\n",
    "        pl.col(\"pseudonhs_2024-07-10\").is_not_null(), # there are some rows with NON-NULL exome_id but NULL pseudo_nhs_number\n",
    "        label=\"Only include NON-NULL exome_id and NON-NULL pseudo_nhs_number for 55k Regenie\"\n",
    "    )\n",
    "    .TRE\n",
    "    .filter_with_logging(\n",
    "        pl.col(\"OrageneID\").is_not_null(),\n",
    "        label=\"Sanity check to ensure no NULL OrageneID. row count should remain unchanged\"\n",
    "    )\n",
    "    .TRE\n",
    "    .unique_with_logging(\n",
    "        [\"pseudonhs_2024-07-10\"],\n",
    "        label=\"Sanity check: row count should remain unchanged when uniquing by pseudo_nhs_number\"\n",
    "    )\n",
    "    .TRE\n",
    "    .unique_with_logging(\n",
    "        [\"OrageneID\"],\n",
    "        label=\"Sanity check: row count should remain unchanged when uniquing by OrageneID\"\n",
    "    )\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899dda1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"mapping\": {\n",
    "        \"OrageneID\": \"study_id\",\n",
    "        \"pseudonhs_2024-07-10\": \"nhs_number\"\n",
    "    },\n",
    "    \"demographics\": {\n",
    "        \"S1QST_Oragene_ID\": \"study_id\",\n",
    "        \"S1QST_MM-YYYY_ofBirth\": \"dob\",\n",
    "        \"S1QST_Gender\": \"gender\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341c4941",
   "metadata": {},
   "source": [
    "### Process the data \n",
    "\n",
    "In this we are processing the data. We then print the first 5 rows to double check that the format is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84c2fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics.process_dataset(column_maps=config, round_to_day_in_month=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a3c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics.log.append(f\"{datetime.now()}: Data cleaned and processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684921f7",
   "metadata": {},
   "source": [
    "### Save the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecef019",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics.write_to_feather(f\"{DEMOGRAPHICS_LOCATION}/clean_demographics.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76869efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics.write_to_log(f\"{DEMOGRAPHICS_LOCATION}/clean_demographics_log.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6e2570",
   "metadata": {},
   "source": [
    "### Run next cell to initiate next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9061f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "redirect_to_next_notebook_in_pipeline(\"2-process-datasets-discovery-primary-care\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cdc750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
